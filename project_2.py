# -*- coding: utf-8 -*-
"""Project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DH49jO_iHlxq-2P9t5-Ow3myzIF-I7ux

# Mini Project 2

Import required libraries
"""

#importing libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

"""Load given datasets"""

train_data = pd.read_csv("C:\\Users\\nimmy\\Downloads\\datasets_o4r_4d0uxogf\\Datasets\\Consumer_Complaints_train.csv")
test_data = pd.read_csv("C:\\Users\\nimmy\\Downloads\\datasets_o4r_4d0uxogf\\Datasets\\Consumer_Complaints_test.csv")

"""Print top 5 records of train dataset"""

train_data.head()

"""Print top 5 records of test dataset"""

test_data.head()

"""Print shape of train and test datasets"""

print(train_data.shape)
print(test_data.shape)

"""Print columns of train and test datasets"""

print(train_data.columns)

print(test_data.columns)

"""Check data type for both datasets"""

train_data.info()

test_data.info()

"""Print missing values in percentage"""

train_data.isnull().sum()

percent_missing = train_data.isnull().sum() * 100 / len(train_data)
missing_value_df = pd.DataFrame({'percent_missing': percent_missing})
missing_value_df

test_data.isnull().sum()

percent_missing1 = test_data.isnull().sum() * 100 / len(test_data)
missing_value_df1 = pd.DataFrame({'percent_missing': percent_missing1})
missing_value_df1

drop_col_train = missing_value_df[missing_value_df['percent_missing']>25]
drop_col_train

drop_col_test = missing_value_df1[missing_value_df['percent_missing']>25]
drop_col_test

drop_col_train= drop_col_train.T
drop_col_test = drop_col_test.T

train_data= train_data.drop( columns= drop_col_train.columns)
test_data= test_data.drop( columns= drop_col_test.columns)

train_data.isnull().sum()

test_data.isnull().sum()

train_data.shape

train_data.info()

train_data.isnull().sum()

train_data.head()

"""Extract Date, Month, and Year from the "Date Received" Column and create new fields for year, month, and day."""

train_data[["Year","Month", "Day"]] = train_data["Date received"].str.split("-", expand = True)
train_data

test_data[["Year","Month", "Day"]] = test_data["Date received"].str.split("-", expand = True)
test_data

"""Convert dates from object type to datetime type"""

train_data['Date received'] = pd.to_datetime(train_data['Date received'])
train_data['Date sent to company'] = pd.to_datetime(train_data['Date sent to company'])

test_data['Date received'] = pd.to_datetime(test_data['Date received'])
test_data['Date sent to company'] = pd.to_datetime(test_data['Date sent to company'])

"""Calculate the number of days the complaint was with the company

create new field with help given logic
Like, Days held = Date sent to company - Date received
"""

train_data['Number of Days held']= train_data['Date sent to company']- train_data['Date received']
train_data.head()

test_data['Number of Days held']= test_data['Date sent to company']- test_data['Date received']
test_data.head()

train_data.info()

"""Convert "Days Held" to Int(above column)"""

import datetime as dt
train_data['Days held'] = train_data['Number of Days held'].map(lambda x: np.nan if pd.isnull(x) else x.days)
train_data.head()

test_data['Days held'] = test_data['Number of Days held'].map(lambda x: np.nan if pd.isnull(x) else x.days)
test_data.head()

train_data.info()

"""Drop "Date Received","Date Sent to Company","ZIP Code", "Complaint ID""""

train_data1= train_data.drop(['Date received','Date sent to company','ZIP code', 'Complaint ID','Number of Days held'], axis=1)
train_data1

test_data1= test_data.drop(['Date received','Date sent to company','ZIP code', 'Complaint ID','Number of Days held'], axis=1)
test_data1

train_data1.info()

train_data.info()

train_data.isnull().sum()

test_data.isnull().sum()

"""Impute null values in "State" by Mode (find mode and replace nan value)"""

test_data.info()

si = SimpleImputer( missing_values=np.nan, strategy='most_frequent')
train_data.iloc[:,4:5] = si.fit_transform(train_data.iloc[:,4:5])

si = SimpleImputer( missing_values=np.nan, strategy='most_frequent')
test_data.iloc[:,4:5] = si.fit_transform(test_data.iloc[:,4:5])

"""Check Missing Values in the dataset"""

train_data.isnull().sum()

test_data.isnull().sum()

"""Categorize Days into Weeks with the help of 'Days Received'"""

train_data.head()

train_data.info()

train_data['formatted_date'] = pd.to_datetime(train_data['Date received'])
train_data['week_of_year'] = train_data.formatted_date.apply(lambda x: x.weekofyear)

test_data['formatted_date'] = pd.to_datetime(test_data['Date received'])
test_data['week_of_year'] = test_data.formatted_date.apply(lambda x: x.weekofyear)

train_data.head()

train_data2= pd.DataFrame(train_data)

test_data2 = pd.DataFrame(test_data)

"""Drop "Day_Received" column"""

train_data= train_data.drop(['Date received','formatted_date','Number of Days held'], axis =1)

test_data= test_data.drop(['Date received','formatted_date','Number of Days held'], axis =1)

train_data.head()

"""Print head of train and test dataset and observe"""

train_data.head()

test_data.head()

"""Store data of the disputed consumer in the new data frame as "disputed_cons""""

train_data['disputed_cons'] = train_data['Consumer disputed?']
train_data.head()

"""Plot bar graph for the total no of disputes with the help of seaborn"""

train_data['disputed_cons'].value_counts()

sns.countplot(x='disputed_cons',data= train_data)
plt.show()

"""Plot bar graph for the total no of disputes products-wise with help of seaborn"""

train_data.groupby('Product')['disputed_cons'].value_counts()

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Product',hue='disputed_cons', data= train_data)
plt.show()

"""Plot bar graph for the total no of disputes with Top Issues by Highest Disputes , with help of seaborn"""

train_data.groupby('Issue')['disputed_cons'].value_counts().sort_values(ascending=False)

train_data['disputed_cons'].value_counts().iloc[:10].index

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Issue',hue='disputed_cons', data= train_data ,order=train_data.Issue.value_counts().iloc[:5].index)
plt.show()

"""Plot bar graph for the total no of disputes by State with Maximum Disputes"""

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'State',hue='disputed_cons', data= train_data ,order=train_data.State.value_counts().iloc[:5].index)
plt.show()

"""Plot bar graph for the total no of disputes by Submitted Via diffrent source"""

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Submitted via',hue='disputed_cons', data= train_data ,order=train_data['Submitted via'].value_counts().iloc[:5].index)
plt.show()

"""Plot bar graph for the total no of disputes wherevCompany's Response to the Complaints"""

train_data['Company response to consumer'].value_counts()

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Company response to consumer',hue='disputed_cons', data= train_data ,order=train_data['Company response to consumer'].value_counts().iloc[:5].index)
plt.show()

"""Plot bar graph for the total no of disputes Whether there are Disputes Instead of Timely Response"""

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Timely response?',hue='disputed_cons', data= train_data ,order=train_data['Timely response?'].value_counts().iloc[:5].index)
plt.show()

"""Plot bar graph for the total no of disputes over Year Wise Complaints"""

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Year',hue='disputed_cons', data= train_data ,order=train_data['Year'].value_counts().iloc[:5].index)
plt.show()

"""Plot bar graph for the top companies with highest complaints"""

plt.figure(figsize=(20,5)) 
sns.countplot(x= 'Company',hue='disputed_cons', data= train_data ,order=train_data['Company'].value_counts().iloc[:5].index)
plt.show()

""""Days Held" Column Analysis(describe)"""

train_data['Days held'].describe()

test_data['Days held'].describe()

"""Convert Negative Days Held to Zero(it is the time taken by authority can't be negative)"""

train_data['Days held']= train_data['Days held'].map(lambda x: 0 if (x<0) else x)
train_data['Days held'].describe()

test_data['Days held']= test_data['Days held'].map(lambda x: 0 if (x<0) else x)
test_data['Days held'].describe()

train_data.head()

"""Text pre-processing (It will be cover in upcoming calsses)"""

!pip install nltk
from nltk.tokenize import wordpunct_tokenize

import string 
import nltk
nltk.download('all')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

relevant_text_train = train_data['Issue']
relevant_text_test = test_data['Issue']
tokenized_data_train = relevant_text_train.apply(lambda x: wordpunct_tokenize(x.lower()))
tokenized_data_test = relevant_text_test.apply(lambda x: wordpunct_tokenize(x.lower()))
def remove_punctuation(text):
    no_punctuation = []
    for w in text:
        if w not in string.punctuation:
            no_punctuation.append(w)
    return no_punctuation
no_punctuation_data_train = tokenized_data_train.apply(lambda x: remove_punctuation(x))
no_punctuation_data_test = tokenized_data_test.apply(lambda x: remove_punctuation(x))
stop_words = stopwords.words('english')
filtered_sentence_train = [w for w in no_punctuation_data_train if not w in stop_words]
filtered_sentence_train = pd.Series(filtered_sentence_train)
filtered_sentence_test = [w for w in no_punctuation_data_test if not w in stop_words]
filtered_sentence_test = pd.Series(filtered_sentence_test)
def lemmatize_text(text):
    lem_text = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in text]
    return lem_text
lemmatized_data_train = filtered_sentence_train.apply(lambda x:lemmatize_text(x))
lemmatized_data_test = filtered_sentence_test.apply(lambda x:lemmatize_text(x))
def stem_text(text):
    stem_text = [PorterStemmer().stem(w) for w in text]
    return stem_text
stemmed_data_train = lemmatized_data_train.apply(lambda x:stem_text(x))
stemmed_data_test = lemmatized_data_test.apply(lambda x:stem_text(x))
def word_to_sentence(text):
    text_sentence = " ".join(text)
    return text_sentence
clean_data_train = stemmed_data_train.apply(lambda x:word_to_sentence(x))
clean_data_test = stemmed_data_test.apply(lambda x:word_to_sentence(x))

train_data['Issues_cleaned'] = clean_data_train
test_data['Issues_cleaned'] = clean_data_test
train_data.head()

train_data = train_data.drop('Issue', axis = 1)
test_data = test_data.drop('Issue', axis = 1)

test_data.isnull().sum()

train_data.isnull().sum()

"""Drop Unnecessary Columns for the Model Building
like:'Company', 'State', 'Year_Received', 'Days_held'
"""

train_data.head()

train_data= train_data.drop( ['Company', 'State', 'Year', 'Days held'], axis=1)
test_data= test_data.drop( ['Company', 'State', 'Year', 'Days held'], axis=1)

train_data.head()

test_data.head()

"""Change Consumer Disputed Column to 0 and 1(yes to 1, and no to 0)"""

le = LabelEncoder()
train_data['Consumer disputed?']= le.fit_transform(train_data['Consumer disputed?'])
train_data.head()

"""Create Dummy Variables for catagorical features like: 'Product', 'Submitted via', 'Company response to consumer', 'Timely response?'"""

train_data.info()

test_data.info()

C_dummy= pd.get_dummies(train_data['Company response to consumer'])
C_dummy= C_dummy.select_dtypes(include=['uint8']).astype('int')
train_data= pd.concat([train_data, C_dummy], axis=1)


C_dummy2= pd.get_dummies(test_data['Company response to consumer'])
C_dummy2= C_dummy2.select_dtypes(include=['uint8']).astype('int')
test_data= pd.concat([test_data, C_dummy2], axis=1)

T_dummy= pd.get_dummies(train_data['Timely response?'])
T_dummy= T_dummy.select_dtypes(include=['uint8']).astype('int')
train_data= pd.concat([train_data, T_dummy], axis=1)

T_dummy2= pd.get_dummies(test_data['Timely response?'])
T_dummy2= T_dummy2.select_dtypes(include=['uint8']).astype('int')
test_data =pd.concat([test_data, T_dummy2], axis=1)

S_dummy= pd.get_dummies(train_data['Submitted via'])
S_dummy= S_dummy.select_dtypes(include=['uint8']).astype('int')
train_data= pd.concat([train_data, S_dummy], axis=1)

S_dummy2= pd.get_dummies(test_data['Submitted via'])
S_dummy2= S_dummy2.select_dtypes(include=['uint8']).astype('int')
test_data= pd.concat([test_data, S_dummy2], axis=1)

Product_dummy= pd.get_dummies(train_data['Product'])
Product_dummy= Product_dummy.select_dtypes(include=['uint8']).astype('int')
train_data= pd.concat([train_data, Product_dummy], axis=1)

Product_dummy2= pd.get_dummies(test_data['Product'])
Product_dummy2= Product_dummy2.select_dtypes(include=['uint8']).astype('int')
test_data= pd.concat([test_data, Product_dummy2], axis=1)

train_data.head()

train_data.info()

test_data.head()

train_data3= pd.DataFrame(train_data)

test_data3 = pd.DataFrame(test_data)

train_data= train_data.drop( ['Product', 'Submitted via', 'Timely response?', 'Company response to consumer'], axis=1)
test_data= test_data.drop( ['Product', 'Submitted via', 'Timely response?', 'Company response to consumer'], axis=1)

train_data.isnull().sum()

test_data.isnull().sum()

"""Calculating TF-IDF"""

train_data4= pd.DataFrame(train_data)
test_data4 = pd.DataFrame(test_data)

from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer()
issues_cleaned_train = tf.fit_transform(train_data['Issues_cleaned']).toarray()
issues_cleaned_test = tf.fit_transform(test_data['Issues_cleaned']).toarray()
tf_columns_train = []
tf_columns_test = []
for i in range(issues_cleaned_train.shape[1]):
    tf_columns_train.append('Feature' + str(i+1))
for i in range(issues_cleaned_test.shape[1]):
    tf_columns_test.append('Feature' + str(i+1))
issues_train = pd.DataFrame(issues_cleaned_train, columns = tf_columns_train)
issues_test = pd.DataFrame(issues_cleaned_test, columns = tf_columns_test)
weights = pd.DataFrame(tf.idf_, index = tf.get_feature_names(), columns = ['Idf_weights']).sort_values(by = 'Idf_weights', ascending = False)
weights.head()

issues_train

issues_train.info()

"""Replacing Issues_cleaned by Vectorized Issues"""

train_data = train_data.drop('Issues_cleaned', axis = 1)
test_data = test_data.drop('Issues_cleaned', axis = 1)
train_data = pd.concat([train_data, issues_train], axis = 1)
test_data = pd.concat([test_data, issues_test], axis = 1)
Feature168 = [0] * 119606
test_data['Feature168'] = Feature168

"""observe train and test datasets"""

train_data.head()

test_data.head()

"""Observe Shape of new Train and Test Datasets"""

train_data.shape

test_data.shape

train_data5= pd.DataFrame(train_data)
test_data5= pd.DataFrame(test_data)

#train_data= pd.DataFrame(train_data5)

"""Scaling the Data Sets (note:discard dependent variable before doing standardization)"""

#train_data= pd.DataFrame(train_data4)
#test_data = pd.DataFrame(test_data4)

train_data.info()

train_data.columns

train_data.info()

print(train_data.dtypes)

train_data.select_dtypes(include=['object'])

train_data.select_dtypes(include=['datetime64[ns]'])

test_data['Month'] = test_data['Month'].astype('int')
test_data['Day'] = test_data['Day'].astype('int')

sc = StandardScaler()
train_data= train_data.drop(['Consumer disputed?','disputed_cons','ZIP code','Date sent to company'], axis=1)
test_data= test_data.drop(['ZIP code','Date sent to company'], axis=1)

train_data.info()

test_data.info()

train_data= sc.fit_transform(train_data)

train_data6= pd.DataFrame(train_data)
test_data6= pd.DataFrame(test_data)

"""Do feature selection with help of PCA.

Select top features which are covering 80% of the information (n=53),
store this data into new dataframe,:
"""

pca = PCA(n_components=70)
principalComponents = pca.fit_transform(train_data)
principalDf = pd.DataFrame(data = principalComponents)

pca1 = PCA(n_components=70)
principalComponents1 = pca1.fit_transform(test_data)
principalDf1 = pd.DataFrame(data = principalComponents1)
test_data= pd.DataFrame(principalDf1)

principalDf.head()

print(pca.explained_variance_ratio_)
print(pca.explained_variance_ratio_.sum())

"""Split the Data Sets Into X and Y by dependent and independent variables (data selected by PCA)"""

X= pd.DataFrame(principalDf)
y= train_data4['Consumer disputed?']

"""Split data into Train and Test datasets (for test data use test excel file data)"""

X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=100)

X_test =  pd.DataFrame(test_data)

"""Shapes of the datasets"""

print(X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape)

"""Model building Build given models and mesure their test and validation accuracy build given models:

1. LogisticRegression
2. DecisionTreeClassifier
3. RandomForestClassifier
4. AdaBoostClassifier
5. GradientBoostingClassifier
6. KNeighborsClassifier
7. XGBClassifier
"""

l = LogisticRegression()
l.fit(X_train, y_train)
y_pred= l.predict(X_val)
accuracy_score_l= accuracy_score(y_pred,y_val)
print(accuracy_score_l)

d= DecisionTreeClassifier()
d.fit(X_train, y_train)
y_pred= l.predict(X_val)
accuracy_score_d= accuracy_score(y_pred,y_val)
print(accuracy_score_d)

r = RandomForestClassifier()
r.fit(X_train, y_train)
y_pred= l.predict(X_val)
accuracy_score_r = accuracy_score(y_pred,y_val)
print(accuracy_score_r)

from sklearn.ensemble import AdaBoostClassifier
ad = AdaBoostClassifier(base_estimator=d)
ad.fit(X_train,y_train)
y_pred = ad.predict(X_val)
accuracy_score_ada= accuracy_score(y_pred,y_val)
print(accuracy_score_ada)

from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)
gb.fit(X_train,y_train)
y_pred = gb.predict(X_val)
accuracy_score_g= accuracy_score(y_pred,y_val)
print(accuracy_score_g)

for i in range(2,11):
  m=0
  k = KNeighborsClassifier(n_neighbors=i)
  k.fit(X_train, y_train)
  y_pred = k.predict(X_val)
  a= accuracy_score(y_pred,y_val)
  if(m<a):
    m=a
    n=i
  print("accuracy:",accuracy_score(y_pred,y_val), "for value of k :",i)
print("accuracy:",m, "for value of k :",n)
accuracy_score_k= m

from xgboost import XGBClassifier
x = XGBClassifier()
x.fit(X_train,y_train)
y_pred = gb.predict(X_val)
accuracy_score_x= accuracy_score(y_pred,y_val)
print(accuracy_score_x)

test_data

"""Final Model and Prediction for test data file"""

predictions = x.predict(X_test)

"""Export Predictions to CSV"""

output = pd.DataFrame({'Customer Disputed': predictions})
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")